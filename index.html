<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face App</title>
    <style>
        #overlay,
        .overlay {
            position: absolute;
            top: 0;
            left: 0;
        }
    </style>

</head>

<body>
    <img src="images/friendsreunion.jpg" id="originalImg" />
    <canvas id="reflay" class="overlay"></canvas>
</body>
<script src="https://code.jquery.com/jquery-3.6.0.min.js">//JQuery 라이브러리 로드</script>
<script src="js/face-api.js"></script>
<script src="js/faceSystem.js"></script>
<script>
    $(document).ready(function () {

        async function face() {

            const MODEL_URL = '/models'

            await faceapi.loadSsdMobilenetv1Model(MODEL_URL)
            await faceapi.loadFaceLandmarkModel(MODEL_URL)
            await faceapi.loadFaceRecognitionModel(MODEL_URL)
            await faceapi.loadFaceExpressionModel(MODEL_URL)

            const img = document.getElementById('originalImg')
            let faceDescriptions = await faceapi.detectAllFaces(img).withFaceLandmarks().withFaceDescriptors().withFaceExpressions()
            const canvas = $('#reflay').get(0)
            faceapi.matchDimensions(canvas, img)

            faceDescriptions = faceapi.resizeResults(faceDescriptions, img)
            // faceapi.draw.drawDetections(canvas, faceDescriptions)
            // faceapi.draw.drawFaceLandmarks(canvas, faceDescriptions)
            // faceapi.draw.drawFaceExpressions(canvas, faceDescriptions)

            faceDescriptions.forEach((faceDescription, i) => {
                const box = faceDescription.detection.box; // 감지된 얼굴의 bounding box 정보를 가져옵니다.
                const x = box.x; // 얼굴의 x 좌표
                const y = box.y; // 얼굴의 y 좌표
                const width = box.width; // 얼굴의 너비
                const height = box.height; // 얼굴의 높이
                console.log(`Face ${i + 1}: x=${x}, y=${y}, width=${width}, height=${height}`);
            })

            // const labels = ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad']

            // const labeledFaceDescriptors = await Promise.all(
            //     labels.map(async label => {

            //         const imgUrl = `images/${label}.jpg`
            //         const img = await faceapi.fetchImage(imgUrl)

            //         const faceDescription = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()

            //         if (!faceDescription) {
            //             throw new Error(`no faces detected for ${label}`)
            //         }

            //         const faceDescriptors = [faceDescription.descriptor]
            //         return new faceapi.LabeledFaceDescriptors(label, faceDescriptors)
            //     })
            // );

            // const threshold = 0.6
            // const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, threshold)

            // const results = faceDescriptions.map(fd => faceMatcher.findBestMatch(fd.descriptor))

            // results.forEach((bestMatch, i) => {
            //     const box = faceDescriptions[i].detection.box
            //     const text = bestMatch.toString()
            //     const drawBox = new faceapi.draw.DrawBox(box, { label: text })
            //     drawBox.draw(canvas)
            // })

        }

        face()
    })
</script>

</html>